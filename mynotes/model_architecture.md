# Nanochat D4 - Transformer Model Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        NANOCHAT D4 - TRANSFORMER MODEL                      │
│                                                                             │
│   Config: n_layer=4, n_head=2, n_kv_head=2, n_embd=256, vocab=65536        │
│   Parameters: ~36.7M                                                        │
└─────────────────────────────────────────────────────────────────────────────┘

                                   Input Tokens
                                        │
                                        ▼
                         ┌──────────────────────────┐
                         │   Token Embedding (wte)  │
                         │   65536 × 256            │
                         └──────────────────────────┘
                                        │
                                        ▼
                         ┌──────────────────────────┐
                         │       RMSNorm            │
                         └──────────────────────────┘
                                        │
                ┌───────────────────────┼───────────────────────┐
                │                       │                       │
                ▼                       ▼                       ▼
         ╔═════════════╗         ╔═════════════╗         ╔═════════════╗
         ║  Block 0    ║         ║  Block 1    ║   ...   ║  Block 3    ║
         ╚═════════════╝         ╚═════════════╝         ╚═════════════╝
                │                       │                       │
                └───────────────────────┼───────────────────────┘
                                        │
                                        ▼

         ╔═══════════════════════════════════════════════════════════════╗
         ║                     TRANSFORMER BLOCK (×4)                     ║
         ╠═══════════════════════════════════════════════════════════════╣
         ║                                                               ║
         ║   Input ──────────────────────┬───────────────────────────►(+)║
         ║                               │                              │ ║
         ║                               ▼                              │ ║
         ║                    ┌──────────────────┐                      │ ║
         ║                    │     RMSNorm      │                      │ ║
         ║                    └──────────────────┘                      │ ║
         ║                               │                              │ ║
         ║                               ▼                              │ ║
         ║         ┌─────────────────────────────────────────┐          │ ║
         ║         │      Causal Self-Attention (GQA)        │          │ ║
         ║         │  ┌─────┐  ┌─────┐  ┌─────┐              │          │ ║
         ║         │  │Q 256│  │K 256│  │V 256│              │          │ ║
         ║         │  │ →2h │  │ →2h │  │ →2h │              │          │ ║
         ║         │  └──┬──┘  └──┬──┘  └──┬──┘              │          │ ║
         ║         │     │        │        │                 │          │ ║
         ║         │     ▼        ▼        │                 │          │ ║
         ║         │  ┌─────────────┐      │                 │          │ ║
         ║         │  │ RoPE + Norm │      │                 │          │ ║
         ║         │  └─────────────┘      │                 │          │ ║
         ║         │     │        │        │                 │          │ ║
         ║         │     ▼        ▼        ▼                 │          │ ║
         ║         │  ┌─────────────────────────┐            │          │ ║
         ║         │  │  Scaled Dot-Product     │            │          │ ║
         ║         │  │  Attention (Causal)     │            │          │ ║
         ║         │  └─────────────────────────┘            │          │ ║
         ║         │              │                          │          │ ║
         ║         │              ▼                          │          │ ║
         ║         │     ┌─────────────────┐                 │          │ ║
         ║         │     │ Output Proj 256 │                 │          │ ║
         ║         │     └─────────────────┘                 │          │ ║
         ║         └─────────────────────────────────────────┘          │ ║
         ║                               │                              │ ║
         ║                               └──────────────────────────────┘ ║
         ║                                                               ║
         ║   ────────────────────────────┬───────────────────────────►(+)║
         ║                               │                              │ ║
         ║                               ▼                              │ ║
         ║                    ┌──────────────────┐                      │ ║
         ║                    │     RMSNorm      │                      │ ║
         ║                    └──────────────────┘                      │ ║
         ║                               │                              │ ║
         ║                               ▼                              │ ║
         ║         ┌─────────────────────────────────────────┐          │ ║
         ║         │                  MLP                    │          │ ║
         ║         │  ┌────────────────────────────────┐     │          │ ║
         ║         │  │ Linear: 256 → 1024 (c_fc)      │     │          │ ║
         ║         │  └────────────────────────────────┘     │          │ ║
         ║         │                   │                     │          │ ║
         ║         │                   ▼                     │          │ ║
         ║         │  ┌────────────────────────────────┐     │          │ ║
         ║         │  │        ReLU² Activation        │     │          │ ║
         ║         │  └────────────────────────────────┘     │          │ ║
         ║         │                   │                     │          │ ║
         ║         │                   ▼                     │          │ ║
         ║         │  ┌────────────────────────────────┐     │          │ ║
         ║         │  │ Linear: 1024 → 256 (c_proj)    │     │          │ ║
         ║         │  └────────────────────────────────┘     │          │ ║
         ║         └─────────────────────────────────────────┘          │ ║
         ║                               │                              │ ║
         ║                               └──────────────────────────────┘ ║
         ║                                                     Output     ║
         ╚═══════════════════════════════════════════════════════════════╝

                                        │
                                        ▼
                         ┌──────────────────────────┐
                         │       RMSNorm            │
                         └──────────────────────────┘
                                        │
                                        ▼
                         ┌──────────────────────────┐
                         │   LM Head (lm_head)      │
                         │   256 → 65536            │
                         │   + Softcap (tanh@15)    │
                         └──────────────────────────┘
                                        │
                                        ▼
                                Output Logits
                                (65536 vocab)


┌─────────────────────────────────────────────────────────────────────────────┐
│ KEY FEATURES:                                                               │
│ • Rotary Position Embeddings (RoPE) - no learned positional embeddings     │
│ • QK Normalization - stabilizes attention                                   │
│ • ReLU² activation in MLP - squared ReLU for sparsity                      │
│ • RMSNorm (no learnable params) - simpler normalization                    │
│ • Group-Query Attention (GQA) - efficient inference                        │
│ • Logit softcap @ 15 - prevents extreme logits                             │
│ • No bias in linear layers                                                  │
│ • Untied embedding/unembedding weights                                      │
└─────────────────────────────────────────────────────────────────────────────┘
```
